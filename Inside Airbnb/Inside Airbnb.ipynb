{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "You will be asked to download data by yourselves and answer several data science questions through this final project. This project has two parts.\n",
    "\n",
    "\n",
    "## Part 1 (15 Points):\n",
    "\n",
    "This part contains 10 different data science questions that you are asked to answer. You need to download the corresponding data and write code to process the data to answer these questions. For each question, you need to provide your code to anser this question, as well as final answers. Each question will be worth 1.5 points in this part. \n",
    "\n",
    "\n",
    "## Final Submission:\n",
    "\n",
    "Your final submission will contain two files:\n",
    "\n",
    "1. The first would be this notebook. You need to provide code for your answers as well as your answers. You also need to provide visualization and executive summary using Markdown in this notebook. \n",
    "\n",
    "2. The second is the html version of this notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "First, you need to download the most recent datasets containing listing and reviews in **Los Angeles** from [insidearibnb.com](http://insideairbnb.com/get-the-data.html). (September,2019,detailed data)You will then need to provide the code as well as answers to the following questions.\n",
    "\n",
    "**To make sure that you downloaded the right dataset, your solution to Question 1 should be:**\n",
    "1. **Number of unique listings = 45053**\n",
    "2. **Number of unique hosts = 26286**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: How many unique listings are there in the LA dataset? How many unique hosts are there?\n",
    "**Question 1 Answer:**\n",
    "\n",
    "The unique number of listings in the LA area is 45053.\n",
    "The unique number of IDs in the LA area is 26286."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique number of listings in the LA area is 45053.\n",
      "The unique number of IDs in the LA area is 26286.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alaaa\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (61,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# You need to provide the code to answer the previous quesiton\n",
    "# (you should also briefly explain your data structure -- \n",
    "# where do you put your data and how you get your data)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_listings = pd.read_csv(\"listings.csv\")\n",
    "\n",
    "#We used .nunique() to get the unique listing number, which is stored in num_unique_listing\n",
    "#We used .nunique() to get the unique host number, which is stored in num_unique_host\n",
    "\n",
    "num_unique_listing = df_listings[\"id\"].nunique()\n",
    "num_unique_host = df_listings[\"host_id\"].nunique()\n",
    "print(\"The unique number of listings in the LA area is %i.\" % num_unique_listing) \n",
    "print(\"The unique number of IDs in the LA area is %i.\" % num_unique_host) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: What is the mean, median, standard deviation, minimum and maximum of the number of listings per host in LA? (for each host, you only need to consider the listings from LA)\n",
    "**Question 2 Answer:**\n",
    "\n",
    "The mean, standard deviation, minimum and maximum of the number of listings per host in LA are 1.71, 3.00, 1.00, 1.00 and 172.00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median       1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    26286.000000\n",
       "mean         1.713954\n",
       "std          2.999183\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          1.000000\n",
       "75%          1.000000\n",
       "max        172.000000\n",
       "Name: calculated_host_listings_count, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You need to provide the code to answer the previous quesiton\n",
    "import pandas as pd\n",
    "\n",
    "df_listings = pd.read_csv(\"listings.csv\")\n",
    "\n",
    "#We first used .drop_duplicates() to get unique rows based on 'host_id', and then used .describe() to get basic statistics\n",
    "num_listing_perhost_summary = df_listings.drop_duplicates(keep=\"first\", subset=[\"host_id\"])[\"calculated_host_listings_count\"].describe()\n",
    "num_listing_perhost_median = df_listings.drop_duplicates(keep=\"first\", subset=[\"host_id\"])[\"calculated_host_listings_count\"].median()\n",
    "print(\"median       %s\" %num_listing_perhost_median)\n",
    "num_listing_perhost_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: What is the average and standard deviation number of listings for a super host versus a non-super host? Does super host or non-super host have more listings on average?\n",
    "**Question 3 Answer:**\n",
    "\n",
    "The average and standard deviation number of listings for a super host versus a non-super host are 1.82, 1.67, 3.18 and 2.93.\\\n",
    "Super host has more listings on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>f</td>\n",
       "      <td>1.671021</td>\n",
       "      <td>2.926198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t</td>\n",
       "      <td>1.824264</td>\n",
       "      <td>3.177677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean       std\n",
       "host_is_superhost                    \n",
       "f                  1.671021  2.926198\n",
       "t                  1.824264  3.177677"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You need to provide the code to answer the previous quesiton\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_listings = pd.read_csv(\"listings.csv\")\n",
    "# We first used the column \"host_is_superhost\" as the indentifier to screen out the two different catagories\n",
    "# We then used the apply function with multiple functions to get the data\n",
    "\n",
    "df_unique = df_listings.drop_duplicates(keep=\"first\", subset=[\"host_id\"])\n",
    "df_unique.groupby(by = 'host_is_superhost').apply\\\n",
    "(lambda x: pd.Series({\"mean\" : np.mean(x[\"calculated_host_listings_count\"]),\\\n",
    "                                                               \"std\" : np.std(x[\"calculated_host_listings_count\"])}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: What are the unique types of host verifications are there? (It doesn't matter if you include \"None\" or empty string as a type)\n",
    "**Question 4 Answer:**\n",
    "\n",
    "The unique types of host verifications are:\\\n",
    " 'email',\\\n",
    " 'phone',\\\n",
    " 'facebook',\\\n",
    " 'reviews',\\\n",
    " 'kba',\\\n",
    " 'jumio',\\\n",
    " 'government_id',\\\n",
    " 'offline_government_id',\\\n",
    " 'selfie',\\\n",
    " 'identity_manual',\\\n",
    " 'work_email',\\\n",
    " 'manual_online',\\\n",
    " 'manual_offline',\\\n",
    " 'google',\\\n",
    " '',\\\n",
    " 'sent_id',\\\n",
    " 'None',\\\n",
    " 'sesame',\\\n",
    " 'sesame_offline',\\\n",
    " 'photographer',\\\n",
    " 'zhima_selfie',\\\n",
    " 'weibo'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22 kinds of verification methods.\n",
      "Methods are listed below:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['email',\n",
       " 'phone',\n",
       " 'facebook',\n",
       " 'reviews',\n",
       " 'kba',\n",
       " 'jumio',\n",
       " 'government_id',\n",
       " 'offline_government_id',\n",
       " 'selfie',\n",
       " 'identity_manual',\n",
       " 'work_email',\n",
       " 'manual_online',\n",
       " 'manual_offline',\n",
       " 'google',\n",
       " '',\n",
       " 'sent_id',\n",
       " 'None',\n",
       " 'sesame',\n",
       " 'sesame_offline',\n",
       " 'photographer',\n",
       " 'zhima_selfie',\n",
       " 'weibo']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You need to provide the code to answer the previous quesiton\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_listings = pd.read_csv(\"listings.csv\")\n",
    "\n",
    "#We used lst_veri to store the unique verification types, and we wrote a for loop to clean the strings and\\\n",
    "#to get the unique types\n",
    "lst_veri = []\n",
    "for i in df_listings[\"host_verifications\"].unique().tolist():\n",
    "    veri = i.strip(\"[\").strip(\"]\").strip(\"'\").split(\"', '\")\n",
    "    for j in veri:\n",
    "        if j not in lst_veri:\n",
    "            lst_veri.append(j)\n",
    "            \n",
    "print(\"There are %i kinds of verification methods.\" %len(lst_veri))\n",
    "print(\"Methods are listed below:\")\n",
    "lst_veri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: What are the five most popular verification types of hosts? For each type of the top five verifications, how many percent of hosts verify that type?\n",
    "**Question 5 Answer:**\n",
    "\n",
    "The five most popular verification types of hosts are phone, email, reviews, government_id and jumio.\\\n",
    "The percent of hosts verifying each type are 99.68%, 93.84%, 68.86%, 58.77%, 43.34% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>phone</td>\n",
       "      <td>26201</td>\n",
       "      <td>0.996766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>email</td>\n",
       "      <td>24666</td>\n",
       "      <td>0.938370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>reviews</td>\n",
       "      <td>18099</td>\n",
       "      <td>0.688541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>government_id</td>\n",
       "      <td>15448</td>\n",
       "      <td>0.587689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>jumio</td>\n",
       "      <td>11392</td>\n",
       "      <td>0.433387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count  percentage\n",
       "phone          26201    0.996766\n",
       "email          24666    0.938370\n",
       "reviews        18099    0.688541\n",
       "government_id  15448    0.587689\n",
       "jumio          11392    0.433387"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You need to provide the code to answer the previous quesiton\n",
    "\n",
    "#Note: before running this chunk, the chunk of question 4 needs to be run first\n",
    "\n",
    "# Created a dictionary to store the count of different verfication methods' appearances\n",
    "dic_veri = {}\n",
    "for i in lst_veri:\n",
    "    if i not in dic_veri.keys():\n",
    "        dic_veri[i] = 0\n",
    "\n",
    "#drop duplicates and store the column 'host_verifications' into variable temp\n",
    "temp = df_listings.drop_duplicates(keep=\"first\", subset = [\"host_id\", \"host_verifications\"])[\"host_verifications\"]\n",
    "\n",
    "#clean variable temp\n",
    "for i in temp:\n",
    "    veri = i.strip(\"[\").strip(\"]\").strip(\"'\").split(\"', '\")\n",
    "    for j in veri:\n",
    "        dic_veri[j] += 1\n",
    "\n",
    "#choose the top 5 verification methods and store them along with the counts into variable dic_veri_top5\n",
    "lst_veri_top5 = sorted(dic_veri.items(), key=lambda x: x[1], reverse=True)[0:5]\n",
    "dic_veri_top5 = {}\n",
    "for i in lst_veri_top5:\n",
    "    dic_veri_top5[i[0]] = i[1]\n",
    "\n",
    "#because there are 26286 unique hosts in total, so we calculate the percentages using counts divided by 26286\n",
    "dic_veri_top5 = pd.DataFrame(pd.Series(dic_veri_top5)).rename(columns={0:'count'})\n",
    "dic_veri_top5['percentage'] = dic_veri_top5['count']/26286\n",
    "\n",
    "#At last, the answer to question 5 is stored in variable dic_veri_top5\n",
    "dic_veri_top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: What is the mean, standard deviation of the average price of a listing in Los Angles from 01/01/2020 to 03/01/2020 (inclusive)? (For listings that are not available in this time, you should not count them in the average price.)\n",
    "\n",
    "**Question 6 Answer:**\n",
    "\n",
    "The mean, standard deviation of the average price of a listing in Los Angles from 01/01/2020 to 03/01/2020 (inclusive) are 257.890021 and 10.209746."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alaaa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mean    257.890021\n",
       "std      10.209746\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You need to provide the code to answer the previous quesiton\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_calendar = pd.read_csv(\"calendar.csv\")\n",
    "#just in case, set column 'date' format\n",
    "df_calendar['date'] = pd.to_datetime(df_calendar['date'])\n",
    "#create filter setting in mask\n",
    "mask = (df_calendar['date']>='2020-01-01') & (df_calendar['date']<='2020-03-01') & (df_calendar['available']=='t')\n",
    "#filter rows and store into df_calendar_range\n",
    "df_calendar_range = df_calendar.loc[mask]\n",
    "#clean column 'price'\n",
    "df_calendar_range['price'] = df_calendar_range['price'].str.replace(\"$\",\"\").str.replace(\",\",\"\")\n",
    "df_calendar_range = df_calendar_range.reset_index(drop=True)\n",
    "#set the format of column 'price' to float\n",
    "df_calendar_range['price'] = df_calendar_range['price'].astype(float)\n",
    "\n",
    "#use numpy to calculate the mean and standard deviation of the mean of prices grouped according to different dates\n",
    "df_calendar_range.groupby(by=['date'])['price'].mean().agg([np.mean, np.std])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: For reviews that are written in Jan, 2016, what are the most popular ten words except stopping words? What about reviews written in March, 2017 and April, 2018?\n",
    "**Hint**: You need to use NTLK stopping words to find the stopping words in Python. You also need to remove punctuation and numbers\n",
    "\n",
    "**Question 7 Answer:**\n",
    "\n",
    "The most popular ten words in **Jan 2016**: great, stay, place, location, clean, house, us, nice, would, host (ordered by counts from high to low).\n",
    "\n",
    "The most popular ten words in **March 2017**: great, place, stay, location, clean, nice, would, host, house, comfortable (ordered by counts from high to low).\n",
    "\n",
    "The most popular ten words in **April 2018**: great, place, stay, location, clean, would, host, nice, la, house (ordered by counts from high to low)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alaaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most popular ten words in Jan 2016:\n",
      "great       5421\n",
      "stay        4707\n",
      "place       4321\n",
      "location    2744\n",
      "clean       2744\n",
      "house       2674\n",
      "us          2645\n",
      "nice        2423\n",
      "would       2416\n",
      "host        2349\n",
      "dtype: int64\n",
      "The most popular ten words in March 2017:\n",
      "great          12253\n",
      "place          11567\n",
      "stay            9508\n",
      "location        5920\n",
      "clean           5478\n",
      "nice            4566\n",
      "would           4565\n",
      "host            4474\n",
      "house           3801\n",
      "comfortable     3797\n",
      "dtype: int64\n",
      "The most popular ten words in April 2018:\n",
      "great       18942\n",
      "place       18728\n",
      "stay        14806\n",
      "location    10089\n",
      "clean        9097\n",
      "would        7398\n",
      "host         6854\n",
      "nice         6836\n",
      "la           5603\n",
      "house        5568\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# You need to provide the code to answer the previous quesiton\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords') #execute when there is no local stopwords resources\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Set the stop words and the punctuations for later filtering\n",
    "stop_words = set(stopwords.words('english')) \n",
    "punctuations = set(string.punctuation)\n",
    "\n",
    "df_reviews = pd.read_csv(\"reviews.csv\")\n",
    "\n",
    "#create a function to get the most popular 10 words in different time; the input is the time filters\n",
    "def get_top10words_review(mask):\n",
    "    \n",
    "    #filter rows\n",
    "    df_reviews_range = df_reviews.loc[mask]\n",
    "    df_reviews_range.reset_index(drop=True, inplace=True)\n",
    "    #create a copy for the convenience of debug\n",
    "    df_reviews_range_temp = df_reviews_range.copy()\n",
    "    \n",
    "    #use iteration to get the sorted sequence of words according to counts\n",
    "    for index, row in df_reviews_range_temp.iterrows():\n",
    "        #the non-stopwords version of different reviews is stored in review_nonstopwords\n",
    "        review_nonstopwords = ' '\n",
    "        #pick out the review in different rows and set the format to be string for later methods\n",
    "        review = str(row[\"comments\"])\n",
    "        \n",
    "        #the non-punctuation version of different reviews is stored in review_nonpunctuations\n",
    "        review_nonpunctuations = ''.join(w.lower() for w in review if w not in punctuations).split(\" \")\n",
    "        \n",
    "        #use a for loop to get rid of stopwords\n",
    "        for w in review_nonpunctuations:\n",
    "            if w not in stop_words:\n",
    "                review_nonstopwords = review_nonstopwords + ' ' + w\n",
    "        \n",
    "        #clean the column 'comments'\n",
    "        df_reviews_range_temp.loc[index, 'comments'] = review_nonstopwords\n",
    "    \n",
    "    #pick out the 10 most popular words, and store the result into count\n",
    "    count = pd.Series(' '.join(df_reviews_range_temp['comments']).lower().split()).value_counts()[:10]\n",
    "    \n",
    "    return count\n",
    "\n",
    "mask_1 = (df_reviews['date']>='2016-01-01') & (df_reviews['date']<='2016-01-31')\n",
    "mask_2 = (df_reviews['date']>='2017-03-01') & (df_reviews['date']<='2017-03-31')\n",
    "mask_3 = (df_reviews['date']>='2018-04-01') & (df_reviews['date']<='2018-04-30')\n",
    "\n",
    "print(\"The most popular ten words in Jan 2016:\" )\n",
    "print(get_top10words_review(mask_1))\n",
    "print(\"The most popular ten words in March 2017:\" )\n",
    "print(get_top10words_review(mask_2))\n",
    "print(\"The most popular ten words in April 2018:\" )\n",
    "print(get_top10words_review(mask_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: Which five zipcode has the highest average listing price and has at least 30 listings (again only considering available dates) between 2020-01-01 to 2020-02-01 (inclusive)? What are these listing prices? What are the number of active listings and number of hosts in these top five zipcodes (a listing is active if it has at least one available date in the calendar data)?\n",
    "\n",
    "**Question 8 Answer:**\\\n",
    "The result is as follows:\n",
    "\n",
    "                            zipcode  price\t       num_active_listings\tnum_hosts\n",
    "                            90210\t1926.469725\t 278\t                173\n",
    "                            90077\t1832.004457\t 74\t                 48\n",
    "                            90265\t1547.648483\t 329\t                221\n",
    "                            90069\t949.920967\t  426\t                306\n",
    "                            91436\t639.542918\t  40\t                 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alaaa\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (61,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\Alaaa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Alaaa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>num_active_listings</th>\n",
       "      <th>num_hosts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>90210</td>\n",
       "      <td>1926.469725</td>\n",
       "      <td>278</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90077</td>\n",
       "      <td>1832.004457</td>\n",
       "      <td>74</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90265</td>\n",
       "      <td>1547.648483</td>\n",
       "      <td>329</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90069</td>\n",
       "      <td>949.920967</td>\n",
       "      <td>426</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91436</td>\n",
       "      <td>639.542918</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               price  num_active_listings  num_hosts\n",
       "zipcode                                             \n",
       "90210    1926.469725                  278        173\n",
       "90077    1832.004457                   74         48\n",
       "90265    1547.648483                  329        221\n",
       "90069     949.920967                  426        306\n",
       "91436     639.542918                   40         33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You need to provide the code to answer the previous quesiton\n",
    "import pandas as pd\n",
    "df_calendar = pd.read_csv(\"calendar.csv\")\n",
    "df_listings = pd.read_csv(\"listings.csv\")\n",
    "\n",
    "#just in case, set column 'date' format\n",
    "df_calendar[\"date\"] = pd.to_datetime(df_calendar[\"date\"])\n",
    "#filter rows and store into df_calendar_range\n",
    "df_calendar_range = df_calendar[(df_calendar['date'] >= \"2020-01-01\") & (df_calendar['date'] <= \"2020-02-01\") & (df_calendar['available'] == \"t\")]\n",
    "#clean column 'zipcode'\n",
    "df_listings['zipcode'] = df_listings['zipcode'].str.strip(\"CA \")\\\n",
    "                                               .str.strip(\"Near \")\\\n",
    "                                               .str.strip(\" -This is the address to our main house. The studio is located behind our house off of Stimson\")\n",
    "\n",
    "#choose needed columns and store them into df_listings_slice, so that codes can run faster\n",
    "df_listings_slice = df_listings[[\"id\",\"zipcode\",\"host_id\"]]\n",
    "\n",
    "#use left merge on df_calendar_range and df_listings_slice, based on listing id; store the result into df_combined\n",
    "df_combined = df_calendar_range.merge(df_listings_slice, how = \"left\", left_on = \"listing_id\", right_on = \"id\")\n",
    "\n",
    "#filter rows with no fewer than 30 listings in different zipcodes\n",
    "df_combined = df_combined.groupby(\"zipcode\").filter(lambda x: x['listing_id'].nunique() >= 30)\n",
    "#clean column 'price'\n",
    "df_combined['price'] = df_combined['price'].str.replace(\"$\", \"\").str.replace(\",\",\"\").astype(float)\n",
    "\n",
    "#calculate the mean of price of different listing ids across different zipcodes\n",
    "df_rank = pd.DataFrame(df_combined.groupby(by=['zipcode', 'listing_id'])['price'].mean())\n",
    "#calculate the mean of price in different zipcodes\n",
    "df_rank = pd.DataFrame(df_rank.groupby(by='zipcode')['price'].mean())\n",
    "#rank the price\n",
    "df_rank.sort_values('price', inplace=True, ascending=False)\n",
    "#choose the 5 zipcodes with highest average price\n",
    "df_rank_top5 = df_rank.head(5)\n",
    "\n",
    "#calculate the number of unique listings across zipcodes\n",
    "df_rank_top5['num_active_listings'] = df_combined.groupby(by='zipcode')['listing_id'].nunique()\n",
    "#calculate the number of unique host ids across zipcodes\n",
    "df_rank_top5['num_hosts'] = df_combined.groupby(by='zipcode')['host_id'].nunique()\n",
    "\n",
    "#the result dataframe is stored in df_rank_top5\n",
    "df_rank_top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Focusing on the data from 2020-01-01 to 2020-04-01 (inclusive), what are five zipcodes that have at least 30 listings and have the largest absolute difference between the average prices on weekends versus the average prices on weekdays? (Weekends = Saturday and Sunday). Please give the zipcodes as well as the listing prices\n",
    "**Question 9 Answer:**\\\n",
    "The result is as follows:\n",
    "\n",
    "                        zipcode  abs_difference\tweekday_pricemean\tweekend_pricemean\t\t\n",
    "                        91384\t22.547185\t     138.430670\t       160.977855\n",
    "                        90265\t20.103059\t     1397.389489\t      1417.492548\n",
    "                        90210\t10.723267\t     1620.193908\t      1630.917175\n",
    "                        91325\t9.428009\t      220.009608\t       229.437617\n",
    "                        90293\t8.973894\t      246.446374\t       255.420268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_difference</th>\n",
       "      <th>weekday_pricemean</th>\n",
       "      <th>weekend_pricemean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>91384</td>\n",
       "      <td>22.547185</td>\n",
       "      <td>138.430670</td>\n",
       "      <td>160.977855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90265</td>\n",
       "      <td>20.103059</td>\n",
       "      <td>1397.389489</td>\n",
       "      <td>1417.492548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90210</td>\n",
       "      <td>10.723267</td>\n",
       "      <td>1620.193908</td>\n",
       "      <td>1630.917175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91325</td>\n",
       "      <td>9.428009</td>\n",
       "      <td>220.009608</td>\n",
       "      <td>229.437617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90293</td>\n",
       "      <td>8.973894</td>\n",
       "      <td>246.446374</td>\n",
       "      <td>255.420268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         abs_difference  weekday_pricemean  weekend_pricemean\n",
       "zipcode                                                      \n",
       "91384         22.547185         138.430670         160.977855\n",
       "90265         20.103059        1397.389489        1417.492548\n",
       "90210         10.723267        1620.193908        1630.917175\n",
       "91325          9.428009         220.009608         229.437617\n",
       "90293          8.973894         246.446374         255.420268"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You need to provide the code to answer the previous quesiton\n",
    "\n",
    "import pandas as pd\n",
    "df_calendar = pd.read_csv(\"calendar.csv\")\n",
    "df_listings = pd.read_csv(\"listings.csv\")\n",
    "\n",
    "#just in case, set column 'date' format\n",
    "df_calendar[\"date\"] = pd.to_datetime(df_calendar[\"date\"])\n",
    "#filter rows and store into df_calendar_range\n",
    "df_calendar_range = df_calendar[(df_calendar['date'] >= \"2020-01-01\") & (df_calendar['date'] <= \"2020-04-01\")]\n",
    "#clean column 'zipcode'\n",
    "df_listings['zipcode'] = df_listings['zipcode'].str.strip(\"CA \")\\\n",
    "                                               .str.strip(\"Near \")\\\n",
    "                                               .str.strip(\" -This is the address to our main house. The studio is located behind our house off of Stimson\")\n",
    "\n",
    "#choose needed columns and store them into df_listings_slice, so that codes can run faster\n",
    "df_listings_slice = df_listings[[\"id\",\"zipcode\",\"host_id\"]]\n",
    "\n",
    "#use left merge on df_calendar_range and df_listings_slice, based on listing id; store the result into df_combined\n",
    "df_combined = df_calendar_range.merge(df_listings_slice, how = \"left\", left_on = \"listing_id\", right_on = \"id\")\n",
    "\n",
    "#filter rows with no fewer than 30 listings in different zipcodes\n",
    "df_combined = df_combined.groupby(\"zipcode\").filter(lambda x: x['listing_id'].nunique() >= 30)\n",
    "#clean column 'price'\n",
    "df_combined['price'] = df_combined['price'].str.replace(\"$\", \"\").str.replace(\",\",\"\").astype(float)\n",
    "\n",
    "#use .dt.dayofweek to find out the day of the week at each date, then store the result in column 'day_inaweek'\n",
    "df_combined['day_inaweek'] = pd.Series(df_combined['date']).dt.dayofweek\n",
    "#use 'day_inaweek' to create a new column 'dayofweek' which tells the english instead of numeric version of days\n",
    "df_combined['dayofweek'] = df_combined['day_inaweek'].apply(lambda x: 'weekday' if x <= 4 else 'weekend')\n",
    "\n",
    "#calculate the mean of different days in a week across different zipcodes\n",
    "df_combined_pricemean = pd.DataFrame(df_combined.groupby(by=['zipcode', 'dayofweek'])['price'].mean()).reset_index()\n",
    "#set out the weekday rows to df_combined_pricemean_weekday\n",
    "df_combined_pricemean_weekday = df_combined_pricemean[df_combined_pricemean['dayofweek']=='weekday']\n",
    "#set out the weekend rows to df_combined_pricemean_weekday\n",
    "df_combined_pricemean_weekend = df_combined_pricemean[df_combined_pricemean['dayofweek']=='weekend']\n",
    "#store the zipcodes into df_combined_difference and set 'zipcode' as index, for later's convenience\n",
    "df_combined_difference = pd.DataFrame(df_combined_pricemean_weekday['zipcode']).set_index('zipcode')\n",
    "#set 'zipcode' as index, for later's convenience\n",
    "df_combined_pricemean_weekday.set_index('zipcode', inplace=True)\n",
    "df_combined_pricemean_weekend.set_index('zipcode', inplace=True)\n",
    "\n",
    "#calculate the difference of average price between weekday and weekend across zipcodes\n",
    "#because the index is zipcode, we don't need to bother corrsponding again\n",
    "df_combined_difference['abs_difference'] = abs(df_combined_pricemean_weekday['price'].astype(float) - \\\n",
    "                                           df_combined_pricemean_weekend['price'].astype(float))\n",
    "\n",
    "#sort the difference based on absolute values of price differences\n",
    "df_combined_difference.sort_values('abs_difference', inplace=True, ascending=False)\n",
    "#add average price of weekdays and weekends across zipcodes into the dataframe\n",
    "df_combined_difference['weekday_pricemean'] = df_combined_pricemean_weekday['price']\n",
    "df_combined_difference['weekend_pricemean'] = df_combined_pricemean_weekend['price']\n",
    "#display the five zipcodes with highest absolute average price differences between weekdays and weekends\n",
    "df_combined_difference.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10: What is the average and standard deviation of the daily total capacity at Airbnb Los Angeles from 2020-01-01 to 2020-04-01? The daily total capacity in a day is the number of beds that are available in that given date. What is the average and standard deviation of the daily price per bed at Airbnb Los Angeles from 2020-01-01 to 2020-04-01?\n",
    "**Question 10 Answer:**\\\n",
    "The total capacity mean is 46995.23,\\\n",
    "The total capacity standard deviation is 4980.64,\\\n",
    "The daily price per bed mean is 113.22,\\\n",
    "The daily price per bed standard deviation is 4.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of daily total capacity is 46995.2283\n",
      "The standard deviation of daily total capacity is 4980.6413\n",
      "The average of daily price per bed is 113.2221\n",
      "The standard deviation of daily price per bed is 4.5497\n"
     ]
    }
   ],
   "source": [
    "# You need to provide the code to answer the previous quesiton\n",
    "\n",
    "import pandas as pd\n",
    "df_calendar = pd.read_csv(\"calendar.csv\")\n",
    "df_listings = pd.read_csv(\"listings.csv\")\n",
    "\n",
    "#just in case, set column 'date' format\n",
    "df_calendar[\"date\"] = pd.to_datetime(df_calendar[\"date\"])\n",
    "#filter rows and store into df_calendar_range\n",
    "df_calendar_range = df_calendar[(df_calendar['date'] >= \"2020-01-01\") & (df_calendar['date'] <= \"2020-04-01\") & (df_calendar['available']=='t')]\n",
    "#choose needed columns and store them into df_listings_slice, so that codes can run faster\n",
    "df_listings_slice = df_listings[[\"id\", \"beds\"]]\n",
    "\n",
    "#use left merge on df_calendar_range and df_listings_slice, based on listing id; store the result into df_combined\n",
    "df_combined = df_calendar_range.merge(df_listings_slice, how = \"left\", left_on = \"listing_id\", right_on = \"id\")\n",
    "#clean column 'price'\n",
    "df_combined['price'] = df_combined['price'].str.replace(\"$\", \"\").str.replace(\",\",\"\").astype(float)\n",
    "\n",
    "#choose needed columns and store them into df_combined, then get rid of rows having no value or zero in column 'beds'\n",
    "df_combined = df_combined[['listing_id', 'date', 'price', 'beds']][df_combined['beds']>0]\n",
    "#calculate total capacity at different dates\n",
    "total_capacity = df_combined.groupby(by='date')['beds'].sum()\n",
    "#calculate price per bed\n",
    "df_combined['price_per_bed'] = df_combined['price']/df_combined['beds']\n",
    "#calculate price per bed at different dates\n",
    "daily_price_per_bed = df_combined.groupby(by='date')['price_per_bed'].mean()\n",
    "\n",
    "print('The average of daily total capacity is %0.4f'%total_capacity.mean())\n",
    "print('The standard deviation of daily total capacity is %0.4f'%total_capacity.std())\n",
    "print('The average of daily price per bed is %0.4f'%daily_price_per_bed.mean())\n",
    "print('The standard deviation of daily price per bed is %0.4f'%daily_price_per_bed.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (15 Points):\n",
    "\n",
    "In this part, you need to find **5** interesting business questions out of the datasets. You then need to write Python code to answer the questions. Last, you need to write a 200-word summary of your answers and business insights you get from ansewr these 5 questions baed on your code. You will be evaluated based on the following criterion:\n",
    "\n",
    "1. You need to ask three business-relevant questions (2 points)\n",
    "2. You need to answer these three questions using Python and the datasets (1 points)\n",
    "3. You have at least **5** graphs to visualize your insights (2 points)\n",
    "4. Your executive summary of your answers are well-written and make sense (5 points)\n",
    "5. Your business insights are very interesting and the report is creative (5 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: What are the top 5 most common amenities in the apartment-for-rent? What about their percentage in the overall listings? What changes should be made to improve the overall expereince for renter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandasql'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-823d83f0a346>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandasql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqldf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf_part2_q1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"listings.csv.gz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gzip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"amenities\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandasql'"
     ]
    }
   ],
   "source": [
    "# Your code to answer Question 1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "\n",
    "df_part2_q1 = pd.read_csv(\"listings.csv.gz\", compression = \"gzip\")[\"amenities\"].to_list()\n",
    "return_dict = {}\n",
    "temp_list = []\n",
    "# Import data and creat empty lists to store and count the number of items appeared in the amenities\n",
    "\n",
    "for column in df_part2_q1:\n",
    "    temp_list = str(column).strip(\"{}\").split(\",\")\n",
    "    for item in temp_list:\n",
    "        if item not in return_dict:\n",
    "            return_dict.update({item:1})\n",
    "        elif item in return_dict:\n",
    "            return_dict[item] += 1\n",
    "# Split and create the string that store the different items that are included in the listings\n",
    "# Turn them into a string with duplicates so that the dictionary-count methods can be used on the lists.\n",
    "# If the item occured in the list, the value of the item will increase by one\n",
    "# Just like the verification problem\n",
    "\n",
    "return_part2_q1_df = pd.Series(return_dict, index = return_dict.keys())\n",
    "return_part2_q1_df = pd.DataFrame(return_part2_q1_df)\n",
    "return_part2_q1_df.columns = [\"num_of_items\"]\n",
    "return_part2_q1_df = return_part2_q1_df.sort_values(\"num_of_items\", axis = 0, ascending = False, inplace = False).head(5)\n",
    "return_part2_q1_df[\"percentage_of_item\"] = return_part2_q1_df[\"num_of_items\"]/len(df_part2_q1)\n",
    "print(return_part2_q1_df)\n",
    "# Turn the dictionary into a series, and then turn it to dataframe of sorting purpose\n",
    "# Get the percentage of the items appearances for numerical purposes\n",
    "\n",
    "%matplotlib inline\n",
    "#Import here plotting here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "return_part2_q1_df_plot = return_part2_q1_df\n",
    "return_part2_q1_df_plot[\"items\"] = return_part2_q1_df.index.values\n",
    "# Used the items as the y axis and created a double barh chart for visulization\n",
    "# The left of the graph is the num_of_items, and the right of the graph is the percentage\n",
    "\n",
    "fig, (ax0,ax1) = plt.subplots(nrows = 1, ncols = 2, sharey = True)\n",
    "return_part2_q1_df_plot.plot(kind = \"barh\", x = \"items\", y = \"num_of_items\",ax = ax0)\n",
    "ax0.set(title = \"Numbers of Items\", xlabel = \"Number of Items\", ylabel = \"Item_name\")\n",
    "return_part2_q1_df_plot.plot(kind = \"barh\", x = \"items\", y = \"percentage_of_item\",ax = ax1)\n",
    "ax1.set(title = \"Perc of Items\", xlabel = \"Percentage\", ylabel = \"Item_name\")\n",
    "fig.suptitle(\"Data about Items in Airbnb\")\n",
    "# Plot the data \n",
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: What is the relationship bewteen price and neiborhood? Which neibourhood has the highest price? Which neibourhood has the lowest price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to answer Question 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Note: here we used the listings.csv having relatively small volumn downloaded from insideairbnb\n",
    "df_part2_q2 = pd.read_csv(\"./small_listings.csv\").loc[:,[\"price\",\"neighbourhood\"]]\n",
    "get_data = lambda x : pd.Series({\"mean\" : np.mean(x), \"std\":np.std(x),\\\n",
    "                                          \"median\" : np.median(x), \"most_expensive\" : max(x), \"cheapest\" : min(x),\\\n",
    "                                        \"more_time_expensive\" : max(x)/min(x)})\n",
    "# Get the price and the neibourhood out of the listings data, and then calculate them and outputted the result into a series\n",
    "\n",
    "temp_df = df_part2_q2.groupby(\"neighbourhood\")[\"price\"].mean()\n",
    "temp_df = pd.DataFrame({\"neighbourhood\" : temp_df.index, \"price\" : temp_df.values})\n",
    "pricing_rank_df = temp_df.sort_values('price', ascending = False, inplace = False)\n",
    "# Sort the dataframe by the average price\n",
    "\n",
    "\n",
    "print(pricing_rank_df.head(5))\n",
    "print(pricing_rank_df.tail(5))\n",
    "# To get the list of the top 5 most expensive neighbourhood \n",
    "# And to get the list of the cheapest neigbourhood\n",
    "\n",
    "detailed_data = get_data(pricing_rank_df.price.to_list())\n",
    "detailed_data = get_data(pricing_rank_df.price.to_list())\n",
    "print(detailed_data)\n",
    "# Above is the result of the overall data of the average price group by neibourhood\n",
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#Import here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plot_df_exp = pd.DataFrame(pricing_rank_df.head(5))\n",
    "plot_df_cheap = pd.DataFrame( pricing_rank_df.tail(5))\n",
    "fig, (ax0, ax1) = plt.subplots(nrows = 1, ncols = 2, figsize = (15,5))\n",
    "plot_df_exp.plot(kind = \"barh\", y = \"price\", x = \"neighbourhood\",ax = ax0, color = \"r\")\n",
    "plot_df_cheap.plot(kind = \"barh\", y = \"price\", x = \"neighbourhood\",ax = ax1, color = \"r\")\n",
    "fig.suptitle(\"Avg Price for Neibourhood from Most Expensive to Cheapest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: What are the percentage discounts of private room against entire rooms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to answer Question 3\n",
    "df_part2_q3 = pd.read_csv(\"./small_listings.csv\").loc[:,[\"price\",\"room_type\" ]]\n",
    "entire_home_apt = []\n",
    "private_room =[]\n",
    "hotel_room = []\n",
    "shared_room = []\n",
    "# Read in the data\n",
    "# Created the empty list to store the prices of different type of the room\n",
    "\n",
    "def get_type(row):\n",
    "    if row.room_type == \"Entire home/apt\":\n",
    "        entire_home_apt.append(row.price)\n",
    "    elif row.room_type == \"Private room\":\n",
    "        private_room.append(row.price)\n",
    "    elif row.room_type == \"Hotel room\":\n",
    "        hotel_room.append(row.price)\n",
    "    elif row.room_type == \"Shared room\":\n",
    "        shared_room.append(row.price)\n",
    "# Created a new functions to indentify the different types of the listings, and then store the pricing information \n",
    "# for calculation purposes\n",
    "\n",
    "df_part2_q3.apply(get_type, axis = 1)\n",
    "comparison_mean = pd.Series({\"entire_home_apt_avg\" : np.mean(entire_home_apt), \"private_room_avg\" : np.mean(private_room),\\\n",
    "                       \"hotel_room_avg\" : np.mean(hotel_room), \"shared_room_avg\" : np.mean(shared_room)})\n",
    "comparison_median = pd.Series({\"entire_home_apt_median\" : np.median(entire_home_apt), \\\n",
    "                               \"private_room_median\" : np.median(private_room),\\\n",
    "                       \"hotel_room_median\" : np.median(hotel_room), \"shared_room_median\" : np.median(shared_room)})\n",
    "print(comparison_mean, comparison_median)\n",
    "# 1. Calculate the average price of different room type to make comparison\n",
    "# 2. Calculate the median price of different room type to make comparison with the average price of the different \n",
    "# room type to make sure some of the data are not over-estimated or under-estimated.\n",
    "# Make calculation on the lists of prices of different room type and then turn them into series\n",
    "\n",
    "%matplotlib inline\n",
    "#Import here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plot_df_mean = pd.DataFrame(comparison_mean)\n",
    "plot_df_median = pd.DataFrame(comparison_median)\n",
    "plot_df_mean.columns = [\"avg_price\"]\n",
    "plot_df_median.columns = [\"median_price\"]\n",
    "fig, (ax0, ax1) = plt.subplots(nrows = 1, ncols = 2, figsize = (15,5))\n",
    "plot_df_mean[\"kinds_of_housing\"] = plot_df_mean.index.values\n",
    "plot_df_median[\"kinds_of_housing\"] = plot_df_median.index.values\n",
    "plot_df_mean.plot(kind = \"barh\", y = \"avg_price\", x = \"kinds_of_housing\",ax = ax0, color = \"r\")\n",
    "plot_df_median.plot(kind = \"barh\", y = \"median_price\", x = \"kinds_of_housing\",ax = ax1, color = \"r\")\n",
    "# Plot the series into the graph\n",
    "# 1. The left is the average price per room type\n",
    "# 2. The right is the median price per room type\n",
    "\n",
    "fig.suptitle(\"Price average in different kinds of room\")\n",
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: What are the 10 most common words appearing in the description of listings which have the top 1% review score? Try to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-89237b2ea316>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Your code to answer Question 4\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set strings, numbers, etc, that I want to get rid of\n",
    "stop_words = set(stopwords.words('english')) \n",
    "#set_temp contains frequent but relatively unimportant words in the description, along with numbers\n",
    "set_temp = set(['room', 'home', 'house', 'kitchen', 'bedroom', 'bathroom', '0', '1', '2', '3', '4', '5', '6'\\\n",
    "               '7', '8', '9'])\n",
    "stop_words = stop_words.union(set_temp)\n",
    "punctuations = set(string.punctuation)\n",
    "\n",
    "df_listing = pd.read_csv(\"listings.csv\")\n",
    "df_listing = df_listing[['id', 'name', 'description', 'review_scores_rating']]\n",
    "\n",
    "#choose rows with review ratings in the top 1%\n",
    "df_listing = df_listing[df_listing['review_scores_rating'] >= df_listing['review_scores_rating'].quantile(0.99)]\n",
    "\n",
    "#create a copy, for the convenience of debug, so that we don't need to read csv file again, which is time-consuming\n",
    "df_listing_temp = df_listing.copy()\n",
    "\n",
    "#clean the 'description' column\n",
    "for index, row in df_listing_temp.iterrows():\n",
    "    description_nonstopwords = ' '\n",
    "    description = str(row[\"description\"])\n",
    "\n",
    "    description_nonpunctuations = ''.join(w.lower() for w in description if w not in punctuations).split(\" \")\n",
    "    for w in description_nonpunctuations:\n",
    "        if w not in stop_words:\n",
    "            description_nonstopwords = description_nonstopwords + ' ' + w\n",
    "    df_listing_temp.loc[index, 'description'] = description_nonstopwords\n",
    "\n",
    "#get the top 10 words\n",
    "top10words = pd.Series(' '.join(df_listing_temp['description']).lower().split()).value_counts()[:10]\n",
    "top10words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top10words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-9a8efe8a36b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#visualize the top 10 words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtop10words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop10words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtop10words_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop10words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop10words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtop10words_wc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtop10words_wc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtop10words_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'top10words' is not defined"
     ]
    }
   ],
   "source": [
    "#visualize the top 10 words\n",
    "top10words = pd.DataFrame(top10words).reset_index().rename(columns={'index': 'word', 0:'count'})\n",
    "top10words_dict = dict(zip(top10words['word'], top10words['count']))\n",
    "top10words_wc = WordCloud(width=800, height=400)\n",
    "top10words_wc.generate_from_frequencies(frequencies=top10words_dict)\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.imshow(top10words_wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Consider the most common number of vistors in a reservation, which is 2. Suppose there is a young couple traveling to LA and have a budget of 100 dollars per night, how would you help them choose the listing to stay? Try to narrow down the scope of choices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge folium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to answer Question 5\n",
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "df_listings = pd.read_csv('listings.csv')\n",
    "\n",
    "#clean price data\n",
    "df_listings['price'] = df_listings['price'].str.replace(\"$\", \"\").str.replace(\",\",\"\").astype(float)\n",
    "#create a copy, for the convenience of debug, so that we don't need to read csv file again, which is time-consuming\n",
    "df_listings_copy = df_listings.copy()\n",
    "\n",
    "#filter rows\n",
    "#choose listings accommodating specifically two visitors\n",
    "df_listings_copy = df_listings_copy[df_listings_copy['accommodates']==2]\n",
    "#choose listings having the top 1% review rating\n",
    "df_listings_copy = df_listings_copy[df_listings_copy['review_scores_rating']>=\\\n",
    "                                    df_listings_copy['review_scores_rating'].quantile(0.99)]\n",
    "#choose listings whose host is superhost\n",
    "df_listings_copy = df_listings_copy[df_listings_copy['host_is_superhost']=='t']\n",
    "#choose listings having prices above median but not exceeding the budget\n",
    "df_listings_copy = df_listings_copy[(df_listings_copy['price']<=100) & \\\n",
    "                                    (df_listings_copy['price']>=df_listings_copy['price'].quantile(0.5))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot the map\n",
    "lats = df_listings_copy['latitude'].tolist()\n",
    "lons = df_listings_copy['longitude'].tolist()\n",
    "locations = list(zip(lats, lons))\n",
    "\n",
    "LA_map = folium.Map(location=[34.0522, -118.2437], zoom_start=11)\n",
    "\n",
    "#create a feature group\n",
    "prices = folium.map.FeatureGroup()\n",
    "#add listing locations to the feature group\n",
    "for lat, lon, in zip(lats, lons):\n",
    "    prices.add_child(\n",
    "        folium.CircleMarker(\n",
    "            [lat, lon],\n",
    "            radius=7, # define how big you want the circle markers to be\n",
    "            color='green',\n",
    "            fill=True,\n",
    "            fill_color='lightblue',\n",
    "            fill_opacity=0.4\n",
    "        )\n",
    "    )\n",
    "\n",
    "#add feature group to map\n",
    "LA_map.add_child(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create labels for the pins\n",
    "labels = list(df_listings_copy['price'])\n",
    "#add labels to the feature group\n",
    "for lat, lon, label in zip(lats, lons, labels):\n",
    "    folium.Marker([lat, lon], popup=label).add_to(LA_map)    \n",
    "\n",
    "LA_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create a heatmap to show the distribution of listings\n",
    "LA_map_2 = folium.Map(location = [34.0522, -118.2437], zoom_start = 10)\n",
    "HeatMap(locations).add_to(LA_map_2)\n",
    "\n",
    "LA_map_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "**(You need to use this cell to write your executive summary)**\n",
    "### Note 1:\n",
    "For the first question, our group try to find the most common amenities in the Airbnb listing in the LA area.\n",
    "Through analyzing the data in the listing column, we found that the top 5 most common items in the listing are Wifi, Essentials, Smoke Detector, Kitchen, and Heating. \n",
    "The percentage of the items are listed above. Yet it is surprising to found that TV has an overall 97.5% in all the listings, above the percentage of Smoke Detector, which is one of the most important items in making sure the safety of the renter. \n",
    "### Recommendation 1:  \n",
    "We recommend the enforcement of installing smoke detectors to all the listings for safety purposes. \n",
    "\n",
    "### Note 2:\n",
    "For the second problem, our team was trying to find details about the average pricing of the Airbnb listing by neighborhood. We grouped the data by using neighborhood, and then take the mean of the price of all the listings. \n",
    "The data is concise and easy to follow:\n",
    "For the top 5 most expensive neighborhood, the average price is around 1500 +- 300. The most costly neighborhood is Rolling Hills, with the price around 1880$ per night, Bei-Air and Beverly following after. \n",
    "The reason is because there are not many apartments around that neighborhood, instead, they are mainly housed with a lot of rooms and carefully decorated. So the price is significantly higher. \n",
    "To the cheapest neighborhood, it is mainly around the ghetto. Most of them are shared rooms or private rooms, for people that are looking for a quick stay instead of a vacation. \n",
    "When it comes to comparison, the most expensive listing in Rolling Hills is 94 times more expensive than the listing in Cudahy. \n",
    "### Recommendation 2:  \n",
    "We suggest that a more detailed recommendation system be created. The system categorizes listings in different price range for people who have made similar past purchase. For people who rent apartment such as the one on Rolling Hilling, it is likely that the customer want a sense of the Hollywood lifestyle. So next time when they go to a different city, they might be interested in the richest and most resemble area of the city.\n",
    "\n",
    "### Note 3:\n",
    "For question 3, we were trying to find the connections between different types of room to rent with the price of the listings. \n",
    "From the result, it is evident that the shared room type of listings has the lowest average price, where the entire home_apt has the highest price. \n",
    "The price difference is pretty big, about eight times the average price. But we have noticed something interesting, the average price for the entire home and apt is possibly over-calculated because all the wealthy neighborhood we mentioned above. So we used another chart that calculated the median of the same kind of listings. \n",
    "Surprising, the median price distribution of the listings is pretty much the same as the average distribution of the listings. \n",
    "It is pretty obvious that the percentage up and down of the different types of listings are fixed. \n",
    "### Recommendation 3: \n",
    "We recommend that when homeowners post their listing over the Airbnb, a suggested price range should pop up so the poster has a general understanding of what price range of his/her posting should fall on, to attract the most customer and to make it most appealing. \n",
    "\n",
    "\n",
    "### Note 4:\n",
    "Across all the times, the 10 most popular words for listings receiving the top 1 percent review scores are private, beach, Hollywood, located, living, parking, restaurants, apartment, space, bed, which indicate the best attractions of LA for visitors such as Hollywood and beach. And for the listings themselves, it is important to have private space and a great location near parking lots and restaurants. Among all the listing type, apartment is the most common instead of condos or others. With these ten words, we can get a general idea of LA: a sunny city close to beaches and Hollywood; a great place to hang out and enjoy different food; visitors traveling here prefer private spaces and will most likely find apartments as the most common type of listings they can find on Airbnb. One interesting thing is, Airbnb starts with the idea of 'sharing space,' but the market itself has chosen to let the private spaces become the most common type of listings. Therefore, the sharing idea of Airbnb may be somewhat unsubstantial; the essential form of this business may be just another version of 'traditional' renting services.\n",
    "### Recommendation 4:\n",
    "For the landlords, it is better to design listings with private spaces and close to attractions, in order to get more competitive.\n",
    "\n",
    "### Note 5:\n",
    "After choosing the rows with accommodations == 2 people, I filtered the rows with top 1 percent review scores, finding out that quantile is already 100 out of 100, indicating many customers would rate highly on the listings. It is quite common that a listing gets full score. In order to narrow down the scope of choices, I chose listings whose hosts are superhosts, which is a common filter that visitors would consider. I then chose rows with price above the median but lower than the budget, and created a map to pin those listings after all these filters on the map. The maps are interactive. By clicking the popups the visitors can see the price, and they can directly find out where the listing is, whether located in the downtown or near the beaches or Hollywood. By drawing the heatmap, we can see that the listings satisfying all these filters mostly gather in the downtown area or near from the beaches. On the other hand, they can also spread across the whole city, so the visitors would always find a place to stay near where they want to visit. An interesting thing is, the frequent appearances of these listings in the downtown area shows the downtown may be actually not a common place for locals to live, so landlords rent them out, which is excellent for visitors. \n",
    "### Recommendation 5:\n",
    "For a couple with a budget of 100 dollars, we give our recommendations displayed in the map. These houses are within the budget, close to attractions, and have superhosts, which are great choices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
